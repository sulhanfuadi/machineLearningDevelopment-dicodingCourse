{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UaJ3JkdiHqEO"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_fHkc9oH_ru",
        "outputId": "79c52dd7-3825-4326-cba4-c476bfdbdfa5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_data = [\n",
        "    'Saya suka makan bakso',\n",
        "    'Bakso enak dan lezat',\n",
        "    'Makanan favorit saya adalah nasi goreng',\n",
        "    'Nasi goreng pedas adalah makanan favorit saya',\n",
        "    'Saya suka makanan manis seperti es krim',\n",
        "]"
      ],
      "metadata": {
        "id": "c4C9pAcLICOR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_data = [word_tokenize(sentence.lower()) for sentence in text_data]"
      ],
      "metadata": {
        "id": "CtNVzox_IEFu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# membangun model Word2Vec menggunakan data teks yang sudah di-tokenisasi\n",
        "model = Word2Vec(sentences=tokenized_data, vector_size=100, window=5, min_count=1, workers=4)"
      ],
      "metadata": {
        "id": "b1e6J3wpIGq2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setelah model dibangun, kita bisa menggunakan vektor kata untuk kata-kata tertentu atau mencari kata-kata yang mirip dengan kata tertentu\n",
        "word_vectors = model.wv\n",
        "\n",
        "similar_words = word_vectors.most_similar('bakso', topn=3)\n",
        "print(\"Kata-kata yang mirip dengan 'bakso':\", similar_words)\n",
        "\n",
        "vector = word_vectors['bakso']\n",
        "print(\"Vektor untuk 'bakso':\", vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPgCEjZlIIh1",
        "outputId": "6792ec69-b865-48b0-beae-51e66012901b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kata-kata yang mirip dengan 'bakso': [('manis', 0.2529163062572479), ('nasi', 0.17018672823905945), ('enak', 0.15006466209888458)]\n",
            "Vektor untuk 'bakso': [-0.00713882  0.00124156 -0.00717766 -0.00224369  0.00371885  0.00583258\n",
            "  0.00119832  0.00210183 -0.00411138  0.00722588 -0.00630644  0.00464789\n",
            " -0.00821918  0.00203677 -0.00497649 -0.00424685 -0.00310906  0.00565491\n",
            "  0.00579776 -0.00497439  0.00077378 -0.0084959   0.00780977  0.00925648\n",
            " -0.00274235  0.0007995   0.00074748  0.00547704 -0.00860589  0.00058358\n",
            "  0.00687019  0.00223141  0.00112457 -0.00932216  0.00848288 -0.0062632\n",
            " -0.00299165  0.00349458 -0.00077282  0.00141124  0.00178217 -0.00682961\n",
            " -0.00972456  0.00904072  0.00619895 -0.00691193  0.00340259  0.00020664\n",
            "  0.00475438 -0.00712046  0.00402629  0.00434812  0.00995727 -0.00447314\n",
            " -0.00138943 -0.00731689 -0.00969748 -0.00908048 -0.00102362 -0.00650396\n",
            "  0.0048507  -0.00616346  0.0025184   0.00073924 -0.00339173 -0.00097928\n",
            "  0.00997817  0.009146   -0.00446089  0.00908287 -0.00564239  0.00593029\n",
            " -0.00309763  0.00343232  0.00301726  0.00690047 -0.00237434  0.00877584\n",
            "  0.00759023 -0.00954767 -0.00800735 -0.00763848  0.0029233  -0.00279572\n",
            " -0.00692899 -0.00812822  0.0083084   0.0019909  -0.00932751 -0.00479288\n",
            "  0.00313647 -0.00471295  0.0052802  -0.00423267  0.00264146 -0.00804574\n",
            "  0.00620901  0.00481829  0.00078651  0.00301266]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dalam contoh di atas, kita mencari kata-kata yang mirip dengan 'bakso' dan mendapatkan vektor representasinya.\n",
        "\n",
        "Jadi, dengan menggunakan Word2Vec, kita bisa melatih model untuk membuat representasi vektor dari kata-kata dalam teks yang berguna pada berbagai tugas NLP."
      ],
      "metadata": {
        "id": "e3bCCEpuwCoT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hasil yang kita dapatkan dari kode tersebut adalah berupa dua hal, yaitu kata-kata yang mirip dengan 'bakso' dan vektor representasi untuk kata 'bakso'.\n",
        "\n",
        "Kata-kata yang mirip dengan 'bakso'\n",
        "Dari hasil pencarian, kita dapatkan tiga kata yang mirip dengan 'bakso', yaitu 'manis', 'nasi', dan 'enak'. Nilai yang tercantum di samping kata-kata tersebut adalah ukuran seberapa miripnya kata-kata tersebut dengan 'bakso'. Semakin tinggi nilai tersebut, semakin mirip kata-kata tersebut dengan 'bakso'. Jadi, dalam kasus ini, 'manis' adalah kata yang paling mirip dengan 'bakso', diikuti oleh 'nasi' dan 'enak'.\n",
        "Vektor untuk 'bakso'\n",
        "Vektor ini adalah representasi matematis dari kata 'bakso' dalam ruang vektor. Setiap angka dalam vektor tersebut mewakili fitur atau atribut tertentu dari kata 'bakso'.\n",
        "\n",
        "Misalnya, nilai pertama mungkin mewakili seberapa sering kata 'bakso' muncul dalam konteks tertentu, nilai kedua mungkin mewakili hubungan kata 'bakso' dengan kata-kata lain, seperti 'makanan' atau 'kuliner' dan seterusnya. Dengan vektor ini, mesin dapat memahami kata 'bakso' secara matematis dan menggunakannya dalam berbagai analisis atau tugas pemrosesan bahasa alami."
      ],
      "metadata": {
        "id": "XlShOP_twUBQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Term Frequency-Inverse Document Frequency (TF-IDF)"
      ],
      "metadata": {
        "id": "w_CxywWtIUXA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF (Term Frequency-Inverse Document Frequency) seperti memberi bobot kepada setiap kata dalam dokumen berdasarkan seberapa sering kata itu muncul pada dokumen itu sendiri (frekuensi kata) dan seberapa umumnya kata itu muncul di seluruh kumpulan dokumen."
      ],
      "metadata": {
        "id": "B4i6V3uswttZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "RH2CLG9HIZ3F"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\n",
        "    \"Saya suka makan bakso\",\n",
        "    \"Bakso enak dan lezat\",\n",
        "    \"Makanan favorit saya adalah nasi goreng\",\n",
        "    \"Nasi goreng pedas adalah makanan favorit saya\",\n",
        "    \"Saya suka makanan manis seperti es krim\",\n",
        "]"
      ],
      "metadata": {
        "id": "sVj6DGKLIcvy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inisialisasi objek TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()"
      ],
      "metadata": {
        "id": "VTaJpK_LIePG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# menghitung TF-IDF dari dokumen-dokumen tersebut\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)"
      ],
      "metadata": {
        "id": "EifQ_RcnIg2S"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Di sini, TF-IDF (Term Frequency-Inverse Document Frequency) menghitung seberapa sering sebuah kata muncul dalam sebuah dokumen, lalu dibandingkan dengan seberapa sering kata tersebut muncul di seluruh koleksi dokumen. Ini membantu untuk menemukan kata-kata yang penting dalam sebuah dokumen."
      ],
      "metadata": {
        "id": "cnRqmt_VxBuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# melihat vocabulary (kata unik) yang dihasilkan oleh TfidfVectorizer\n",
        "print(\"Vocabulary:\", tfidf_vectorizer.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaILs-wqIknO",
        "outputId": "e947578f-8b29-4eaf-f9c0-25b16598726e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: {'saya': 14, 'suka': 16, 'makan': 9, 'bakso': 1, 'enak': 3, 'dan': 2, 'lezat': 8, 'makanan': 10, 'favorit': 5, 'adalah': 0, 'nasi': 12, 'goreng': 6, 'pedas': 13, 'manis': 11, 'seperti': 15, 'es': 4, 'krim': 7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# melihat hasil dari TF-IDF matrix dalam bentuk array\n",
        "print(\"TF-IDF Matrix:\")\n",
        "print(tfidf_matrix.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Az-LcwgtInNl",
        "outputId": "07002055-c9bc-4079-95f1-4ab2e7be4225"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Matrix:\n",
            "[[0.         0.49851188 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.61789262 0.         0.\n",
            "  0.         0.         0.34810993 0.         0.49851188]\n",
            " [0.         0.42224214 0.52335825 0.52335825 0.         0.\n",
            "  0.         0.         0.52335825 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.        ]\n",
            " [0.43951606 0.         0.         0.         0.         0.43951606\n",
            "  0.43951606 0.         0.         0.         0.36483803 0.\n",
            "  0.43951606 0.         0.30691325 0.         0.        ]\n",
            " [0.38596041 0.         0.         0.         0.         0.38596041\n",
            "  0.38596041 0.         0.         0.         0.320382   0.\n",
            "  0.38596041 0.47838798 0.26951544 0.         0.        ]\n",
            " [0.         0.         0.         0.         0.42966246 0.\n",
            "  0.         0.42966246 0.         0.         0.28774996 0.42966246\n",
            "  0.         0.         0.24206433 0.42966246 0.34664897]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dengan TF-IDF Matrix ini, kita bisa melihat kata-kata yang paling penting dalam setiap dokumen berdasarkan konteksnya. Semakin tinggi nilai dalam tabel, semakin penting kata tersebut pada dokumen tersebut.\n"
      ],
      "metadata": {
        "id": "rnUn5j2SxO2M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bag of Words (BoW)"
      ],
      "metadata": {
        "id": "KugOxk64Iub_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bag of Words (BoW) adalah pendekatan sederhana dalam pemrosesan teks yang mengubah teks menjadi representasi numerik. Ide dasarnya adalah kita menganggap setiap dokumen sebagai \"tas\" (bag) kata-kata dan hanya peduli tentang keberadaan kata-kata dalam dokumen tersebut, bukan urutan atau konteksnya. Kemudian, untuk setiap dokumen, kita hitung berapa kali setiap kata muncul.\n",
        "\n"
      ],
      "metadata": {
        "id": "F-y9EIOwxWJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "fAz0-IR4ItBq"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\n",
        "    \"Ini adalah contoh dokumen pertama.\",\n",
        "    \"Ini adalah dokumen kedua.\",\n",
        "    \"Ini adalah dokumen ketiga.\",\n",
        "    \"Ini adalah contoh contoh contoh.\"\n",
        "]"
      ],
      "metadata": {
        "id": "W5p3pSsyIz3r"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inisialisasi objek CountVectorizer\n",
        "vectorizer = CountVectorizer()"
      ],
      "metadata": {
        "id": "2F8Vkp4wI1fr"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# melakukan fitting dan transformasi pada data teks menggunakan CountVectorizer (menghitung frekuensi kemunculan setiap kata dalam setiap dokumen)\n",
        "bow_matrix = vectorizer.fit_transform(documents)"
      ],
      "metadata": {
        "id": "vfH_9jZVI3FG"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow_matrix.toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mB9_Px66JEpa",
        "outputId": "31ac22b1-133e-4237-93cb-bfc27c3c1604"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 1, 1, 1, 0, 0, 1],\n",
              "       [1, 0, 1, 1, 1, 0, 0],\n",
              "       [1, 0, 1, 1, 0, 1, 0],\n",
              "       [1, 3, 0, 1, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kita mendapatkan matriks Bag of Words (BoW), yang merupakan representasi numerik dari teks. Matriks ini berisi jumlah kemunculan setiap kata dalam setiap dokumen."
      ],
      "metadata": {
        "id": "HnwokqIWx1VX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = vectorizer.get_feature_names_out()\n",
        "features"
      ],
      "metadata": {
        "id": "A6YK3sPTJGqj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7df9678a-4218-45a2-8bd2-9db6bef3e4db"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['adalah', 'contoh', 'dokumen', 'ini', 'kedua', 'ketiga', 'pertama'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kita juga mendapatkan daftar fitur (kata-kata) yang dihasilkan oleh CountVectorizer.\n"
      ],
      "metadata": {
        "id": "ExNuebcgx-I3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Daftar fitur adalah daftar kata-kata unik yang diurutkan secara alfabetis."
      ],
      "metadata": {
        "id": "98sfb1KsyO8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Matriks BoW:\")\n",
        "print(bow_matrix.toarray())\n",
        "\n",
        "print(\"\\nDaftar Fitur:\")\n",
        "print(features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSH9Jej2JN-6",
        "outputId": "d7084a5f-acdf-48f5-87ff-0c44574c9d6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriks BoW:\n",
            "[[1 1 1 1 0 0 1]\n",
            " [1 0 1 1 1 0 0]\n",
            " [1 0 1 1 0 1 0]\n",
            " [1 3 0 1 0 0 0]]\n",
            "\n",
            "Daftar Fitur:\n",
            "['adalah' 'contoh' 'dokumen' 'ini' 'kedua' 'ketiga' 'pertama']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "N-gram"
      ],
      "metadata": {
        "id": "FqAfccAoJYFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams"
      ],
      "metadata": {
        "id": "iOLNmELNJbQj"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    \"Saya suka makan bakso enak di warung dekat rumah.\",\n",
        "    \"Nasi goreng adalah salah satu makanan favorit saya.\",\n",
        "    \"Es krim coklat sangat lezat dan menyegarkan.\",\n",
        "    \"Saat hari hujan, saya suka minum teh hangat.\",\n",
        "    \"Pemandangan pegunungan di pagi hari sangat indah.\",\n",
        "    \"Bola basket adalah olahraga favorit saya sejak kecil.\"\n",
        "]"
      ],
      "metadata": {
        "id": "JaExvYL4Jd86"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# iterasi melalui setiap kalimat dalam daftar kalimat\n",
        "for sentence in sentences:\n",
        "  # bagi setiap kalimat menjadi kata-kata individu\n",
        "  words = sentence.split()\n",
        "  # gunakan fungsi ngrams untuk menghasilkan n-gram dari kata-kata tersebut\n",
        "  unigrams = list(ngrams(words, 1))\n",
        "  bigrams = list(ngrams(words, 2))\n",
        "  trigrams = list(ngrams(words, 3))"
      ],
      "metadata": {
        "id": "wybXVA3mJgdS"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cetak hasil untuk setiap kalimat, termasuk unigram, bigram, dan trigram.\n",
        "print(\"\\nKalimat:\", sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJ00WSw1JhNi",
        "outputId": "e4226dc9-237a-4868-8b0e-41e275d39e7d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Kalimat: Bola basket adalah olahraga favorit saya sejak kecil.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"1-gram:\")\n",
        "for gram in unigrams:\n",
        "    print(gram)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmNv0_RTJtaY",
        "outputId": "2811715a-6c5f-4eb3-81c9-8504ed0722ce"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1-gram:\n",
            "('Bola',)\n",
            "('basket',)\n",
            "('adalah',)\n",
            "('olahraga',)\n",
            "('favorit',)\n",
            "('saya',)\n",
            "('sejak',)\n",
            "('kecil.',)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n2-gram:\")\n",
        "for gram in bigrams:\n",
        "    print(gram)"
      ],
      "metadata": {
        "id": "_HOpiz8BJyRA",
        "outputId": "493e31fb-7080-4996-d3ff-67b5365e71c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "2-gram:\n",
            "('Bola', 'basket')\n",
            "('basket', 'adalah')\n",
            "('adalah', 'olahraga')\n",
            "('olahraga', 'favorit')\n",
            "('favorit', 'saya')\n",
            "('saya', 'sejak')\n",
            "('sejak', 'kecil.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n3-gram:\")\n",
        "for gram in trigrams:\n",
        "    print(gram)"
      ],
      "metadata": {
        "id": "PGFSt7cqJzyQ",
        "outputId": "d7f1a6bf-5f0a-4d1b-a1b2-960eb0e09d95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "3-gram:\n",
            "('Bola', 'basket', 'adalah')\n",
            "('basket', 'adalah', 'olahraga')\n",
            "('adalah', 'olahraga', 'favorit')\n",
            "('olahraga', 'favorit', 'saya')\n",
            "('favorit', 'saya', 'sejak')\n",
            "('saya', 'sejak', 'kecil.')\n"
          ]
        }
      ]
    }
  ]
}